{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYLlZWNcO7H0XzGDhWT4kU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_Nj-bg2ygDzS"},"outputs":[],"source":["#from functions import get_file_name, impute_with_most_common, handle_missing_categorical_columns, impute_abs_negative_values, string_indexer_transform, inner_join, nombre_columnas, vector_assembler, scaler\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","from pyspark.sql import Row"]},{"cell_type":"code","source":["# Define la prueba para get_file_name\n","def test_get_file_name():\n","    file_path = \"/ruta/al/archivo.csv\"\n","    result = get_file_name(file_path)\n","    expected = \"archivo\"\n","    assert result == expected"],"metadata":{"id":"ycEemTkZjFQy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define la prueba para impute_with_most_common\n","def test_impute_with_most_common(spark_session):\n","    # Crea un DataFrame de prueba con valores nulos\n","    data = [(\"A\", 1), (\"B\", None), (\"C\", 3), (\"A\", 4), (\"B\", None), (\"C\", 6)]\n","    columns = [\"common_column\", \"value\"]\n","    mock_df = spark_session.createDataFrame(data, columns)\n","\n","    # Llama a la función impute_with_most_common con el DataFrame de prueba\n","    result_df = impute_with_most_common(mock_df, \"value\")\n","\n","    # Utiliza afirmaciones (assert) para verificar que los valores nulos han sido reemplazados correctamente\n","    expected_data = [(\"A\", 1), (\"B\", 1), (\"C\", 3), (\"A\", 4), (\"B\", 1), (\"C\", 6)]\n","    expected_columns = [\"common_column\", \"value\"]\n","    expected_result_df = spark_session.createDataFrame(expected_data, expected_columns)\n","\n","    assert result_df.collect() == expected_result_df.collect()"],"metadata":{"id":"5wxy1qZnjKhv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define la prueba para handle_missing_categorical_columns\n","def test_handle_missing_categorical_columns(spark_session):\n","    # Crea un DataFrame de prueba con valores nulos en columnas categóricas\n","    data = [(\"A\", \"X\", 1), (\"B\", None, 2), (\"C\", \"Z\", 3), (\"A\", None, 4), (\"B\", \"X\", 5), (\"C\", \"Z\", 6)]\n","    columns = [\"col1\", \"col2\", \"value\"]\n","    mock_df = spark_session.createDataFrame(data, columns)\n","\n","    # Llama a la función handle_missing_categorical_columns con el DataFrame de prueba\n","    result_df = handle_missing_categorical_columns(mock_df, [\"col2\"])\n","\n","    # Utiliza afirmaciones (assert) para verificar que los valores nulos han sido reemplazados correctamente\n","    expected_data = [(\"A\", \"X\", 1), (\"B\", \"Unknown\", 2), (\"C\", \"Z\", 3), (\"A\", \"Unknown\", 4), (\"B\", \"X\", 5), (\"C\", \"Z\", 6)]\n","    expected_columns = [\"col1\", \"col2\", \"value\"]\n","    expected_result_df = spark_session.createDataFrame(expected_data, expected_columns)\n","\n","    assert result_df.collect() == expected_result_df.collect()"],"metadata":{"id":"xLQIOphWjRsH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define la prueba para handle_missing_categorical_columns\n","def test_handle_missing_categorical_columns(spark_session):\n","    # Crea un DataFrame de prueba con valores nulos en columnas categóricas\n","    data = [(\"A\", \"X\", 1), (\"B\", None, 2), (\"C\", \"Z\", 3), (\"A\", None, 4), (\"B\", \"X\", 5), (\"C\", \"Z\", 6)]\n","    columns = [\"col1\", \"col2\", \"value\"]\n","    mock_df = spark_session.createDataFrame(data, columns)\n","\n","    # Llama a la función handle_missing_categorical_columns con el DataFrame de prueba\n","    result_df = handle_missing_categorical_columns(mock_df, [\"col2\"])\n","\n","    # Utiliza afirmaciones (assert) para verificar que los valores nulos han sido reemplazados correctamente\n","    expected_data = [(\"A\", \"X\", 1), (\"B\", \"Unknown\", 2), (\"C\", \"Z\", 3), (\"A\", \"Unknown\", 4), (\"B\", \"X\", 5), (\"C\", \"Z\", 6)]\n","    expected_columns = [\"col1\", \"col2\", \"value\"]\n","    expected_result_df = spark_session.createDataFrame(expected_data, expected_columns)\n","\n","    assert result_df.collect() == expected_result_df.collect()\n","\n","# Ejecuta la prueba\n","test_handle_missing_categorical_columns(spark)"],"metadata":{"id":"K1LpRypmjmAI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define la prueba para impute_abs_negative_values\n","def test_impute_abs_negative_values(spark_session):\n","    # Crea un DataFrame de prueba con valores negativos\n","    data = [(\"A\", -1.0, 1), (\"B\", 2.0, -2), (\"C\", 3.0, 3), (\"A\", -4.0, 4), (\"B\", 5.0, -5), (\"C\", 6.0, 6)]\n","    columns = [\"col1\", \"col2\", \"value\"]\n","    mock_df = spark_session.createDataFrame(data, columns)\n","\n","    # Llama a la función impute_abs_negative_values con el DataFrame de prueba\n","    result_df = impute_abs_negative_values(mock_df, [\"col2\", \"value\"])\n","\n","    # Utiliza afirmaciones (assert) para verificar que los valores negativos han sido reemplazados correctamente\n","    expected_data = [(\"A\", 1.0, 1), (\"B\", 2.0, 2), (\"C\", 3.0, 3), (\"A\", 4.0, 4), (\"B\", 5.0, 5), (\"C\", 6.0, 6)]\n","    expected_columns = [\"col1\", \"col2\", \"value\"]\n","    expected_result_df = spark_session.createDataFrame(expected_data, expected_columns)\n","\n","    assert result_df.collect() == expected_result_df.collect()\n","\n","# Ejecuta la prueba\n","test_impute_abs_negative_values(spark)\n"],"metadata":{"id":"P3o-87wYjoyi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define la prueba para string_indexer_transform\n","def test_string_indexer_transform(spark_session):\n","    # Crea un DataFrame de prueba con columnas categóricas\n","    data = [(\"A\", \"X\", 1), (\"B\", \"Y\", 2), (\"C\", \"Z\", 3), (\"A\", \"X\", 4), (\"B\", \"Y\", 5), (\"C\", \"Z\", 6)]\n","    columns = [\"col1\", \"col2\", \"value\"]\n","    mock_df = spark_session.createDataFrame(data, columns)\n","\n","    # Llama a la función string_indexer_transform con el DataFrame de prueba\n","    result_df = string_indexer_transform(mock_df, [\"col1\", \"col2\"])\n","\n","    # Utiliza afirmaciones (assert) para verificar que las columnas categóricas han sido transformadas correctamente\n","    expected_data = [(0.0, 0.0, 1), (1.0, 1.0, 2), (2.0, 2.0, 3), (0.0, 0.0, 4), (1.0, 1.0, 5), (2.0, 2.0, 6)]\n","    expected_columns = [\"col1_index\", \"col2_index\", \"value\"]\n","    expected_result_df = spark_session.createDataFrame(expected_data, expected_columns)\n","\n","    assert result_df.collect() == expected_result_df.collect()\n","\n","\n","# Ejecuta la prueba\n","test_string_indexer_transform(spark)"],"metadata":{"id":"uEetKuS3mgjI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define la prueba para inner_join\n","def test_inner_join(spark_session):\n","    # Crea dos DataFrames de prueba para realizar el inner join\n","    data_1 = [(\"A\", 1), (\"B\", 2), (\"C\", 3)]\n","    columns_1 = [\"common_column\", \"value_1\"]\n","    mock_df_1 = spark_session.createDataFrame(data_1, columns_1)\n","\n","    data_2 = [(\"A\", 10), (\"B\", 20), (\"D\", 30)]\n","    columns_2 = [\"common_column\", \"value_2\"]\n","    mock_df_2 = spark_session.createDataFrame(data_2, columns_2)\n","\n","    # Llama a la función inner_join con los DataFrames de prueba\n","    result_df = inner_join(mock_df_1, mock_df_2)\n","\n","    # Utiliza afirmaciones (assert) para verificar que el inner join se realiza correctamente\n","    expected_data = [(\"A\", 1, 10), (\"B\", 2, 20)]\n","    expected_columns = [\"common_column\", \"value_1\", \"value_2\"]\n","    expected_result_df = spark_session.createDataFrame(expected_data, expected_columns)\n","\n","    assert result_df.collect() == expected_result_df.collect()\n","\n","\n","\n","# Ejecuta la prueba\n","test_inner_join(spark)"],"metadata":{"id":"tgreVAjWmyvs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define la prueba para nombre_columnas\n","def test_nombre_columnas(spark_session):\n","    # Crea un DataFrame de prueba con columnas que serán renombradas\n","    data = [(\"A\", 1, 10), (\"B\", 2, 20), (\"C\", 3, 30)]\n","    columns = [\"Ind_ID\", \"CHILDREN\", \"Mobile_phone\"]\n","    mock_df = spark_session.createDataFrame(data, columns)\n","\n","    # Llama a la función nombre_columnas con el DataFrame de prueba\n","    result_df = nombre_columnas(mock_df)\n","\n","    # Utiliza afirmaciones (assert) para verificar que las columnas son renombradas correctamente\n","    expected_data = [(\"A\", 1, 10), (\"B\", 2, 20), (\"C\", 3, 30)]\n","    expected_columns = [\"ID\", \"X1\", \"X2\"]\n","    expected_result_df = spark_session.createDataFrame(expected_data, expected_columns)\n","\n","    assert result_df.collect() == expected_result_df.collect()\n","\n","# Ejecuta la prueba\n","test_nombre_columnas(spark)"],"metadata":{"id":"fZjzAExum2a_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define la prueba para vector_assembler\n","def test_vector_assembler(spark_session):\n","    # Crea un DataFrame de prueba con columnas para ensamblar en un vector\n","    data = [(1, 2, 3, 4, 5), (6, 7, 8, 9, 10), (11, 12, 13, 14, 15)]\n","    columns = [\"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\", \"feature_5\"]\n","    mock_df = spark_session.createDataFrame(data, columns)\n","\n","    # Llama a la función vector_assembler con el DataFrame de prueba\n","    result_df = vector_assembler(mock_df, input_cols=[\"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\", \"feature_5\"])\n","\n","    # Utiliza afirmaciones (assert) para verificar que la columna \"features\" ha sido creada correctamente\n","    expected_data = [(1, 2, 3, 4, 5, [1.0, 2.0, 3.0, 4.0, 5.0]),\n","                     (6, 7, 8, 9, 10, [6.0, 7.0, 8.0, 9.0, 10.0]),\n","                     (11, 12, 13, 14, 15, [11.0, 12.0, 13.0, 14.0, 15.0])]\n","    expected_columns = [\"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\", \"feature_5\", \"features\"]\n","    expected_result_df = spark_session.createDataFrame(expected_data, expected_columns)\n","\n","    assert result_df.collect() == expected_result_df.collect()\n","\n","\n","# Ejecuta la prueba\n","test_vector_assembler(spark)\n"],"metadata":{"id":"wdoRs6GonfjR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define la prueba para scaler\n","def test_scaler(spark_session):\n","    # Crea un DataFrame de prueba con una columna para escalar\n","    data = [(1.0, 2.0, 3.0), (4.0, 5.0, 6.0), (7.0, 8.0, 9.0)]\n","    columns = [\"feature_1\", \"feature_2\", \"feature_3\"]\n","    mock_df = spark_session.createDataFrame(data, columns)\n","\n","    # Llama a la función scaler con el DataFrame de prueba\n","    result_df = scaler(mock_df, input_col=\"feature_1\", output_col=\"scaled_feature_1\")\n","\n","    # Utiliza afirmaciones (assert) para verificar que la columna \"scaled_feature_1\" ha sido creada correctamente\n","    expected_data = [(1.0, 2.0, 3.0, 0.0),\n","                     (4.0, 5.0, 6.0, 0.5),\n","                     (7.0, 8.0, 9.0, 1.0)]\n","    expected_columns = [\"feature_1\", \"feature_2\", \"feature_3\", \"scaled_feature_1\"]\n","    expected_result_df = spark_session.createDataFrame(expected_data, expected_columns)\n","\n","    assert result_df.collect() == expected_result_df.collect()\n","\n","# Ejecuta la prueba\n","test_scaler(spark)\n"],"metadata":{"id":"VHkzkOornlq4"},"execution_count":null,"outputs":[]}]}